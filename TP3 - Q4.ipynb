{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "330b0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "import matplotlib\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f1580e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv('Data/u.csv', sep='|')\n",
    "users_df.columns = ['user_id','age','gender','job','zip']\n",
    "movies_df  = pd.read_csv('Data/items.csv', sep='|')\n",
    "movies_df.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url', 'unknown',  \"Action\",\n",
    "    \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\",\n",
    "    \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
    "ratings_df = pd.read_csv('Data/votes.csv', sep='|')\n",
    "ratings_df.columns = ['user_id', 'movie_id', 'rating', 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dffed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df[\"user_id\"] = users_df[\"user_id\"].apply(lambda x: f\"user_id_{x}\")\n",
    "users_df[\"age\"] = users_df[\"age\"].apply(lambda x: f\"age_{x}\")\n",
    "users_df[\"job\"] = users_df[\"job\"].apply(lambda x: f\"job_{x}\")\n",
    "\n",
    "movies_df[\"movie_id\"] = movies_df[\"movie_id\"].apply(lambda x: f\"movie_id_{x}\")\n",
    "\n",
    "ratings_df[\"movie_id\"] = ratings_df[\"movie_id\"].apply(lambda x: f\"movie_id_{x}\")\n",
    "ratings_df[\"user_id\"] = ratings_df[\"user_id\"].apply(lambda x: f\"user_id_{x}\")\n",
    "ratings_df[\"rating\"] = ratings_df[\"rating\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1b95712",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_group = ratings_df.sort_values(by=[\"timestamp\"]).groupby(\"user_id\")\n",
    "\n",
    "ratings_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": list(ratings_group.groups.keys()),\n",
    "        \"movie_id\": list(ratings_group.movie_id.apply(list)),\n",
    "        \"rating\": list(ratings_group.rating.apply(list)),\n",
    "        \"timestamp\": list(ratings_group.timestamp.apply(list)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9057167",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 2\n",
    "step_size = 1\n",
    "\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        if len(seq) < window_size:\n",
    "            seq = values[-window_size:]\n",
    "            if len(seq) == window_size:\n",
    "                sequences.append(seq)\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences\n",
    "\n",
    "\n",
    "ratings_data.movie_id = ratings_data.movie_id.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "ratings_data.rating = ratings_data.rating.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "del ratings_data[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0648b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id         sequence_movie_ids sequence_ratings     age gender  \\\n",
      "0  user_id_1  movie_id_168,movie_id_172          5.0,5.0  age_24      M   \n",
      "1  user_id_1  movie_id_172,movie_id_165          5.0,5.0  age_24      M   \n",
      "2  user_id_1  movie_id_165,movie_id_156          5.0,4.0  age_24      M   \n",
      "3  user_id_1  movie_id_156,movie_id_166          4.0,5.0  age_24      M   \n",
      "4  user_id_1  movie_id_166,movie_id_196          5.0,5.0  age_24      M   \n",
      "\n",
      "              job  \n",
      "0  job_technician  \n",
      "1  job_technician  \n",
      "2  job_technician  \n",
      "3  job_technician  \n",
      "4  job_technician  \n"
     ]
    }
   ],
   "source": [
    "ratings_data_movies = ratings_data[[\"user_id\", \"movie_id\"]].explode(\n",
    "    \"movie_id\", ignore_index=True\n",
    ")\n",
    "\n",
    "ratings_data_rating = ratings_data[[\"rating\"]].explode(\"rating\", ignore_index=True)\n",
    "\n",
    "ratings_data_transformed = []\n",
    "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
    "\n",
    "ratings_data_transformed = ratings_data_transformed.join(\n",
    "    users_df.set_index(\"user_id\"), on=\"user_id\"\n",
    ")\n",
    "\n",
    "ratings_data_transformed.movie_id = ratings_data_transformed.movie_id.apply(lambda x: \",\".join(x))\n",
    "\n",
    "ratings_data_transformed.rating = ratings_data_transformed.rating.apply(lambda x: \",\".join([str(v) for v in x]))\n",
    "\n",
    "del ratings_data_transformed[\"zip\"]\n",
    "\n",
    "ratings_data_transformed.rename(\n",
    "    columns={\"movie_id\": \"sequence_movie_ids\", \"rating\": \"sequence_ratings\"},\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "print(ratings_data_transformed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0daa4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.80\n",
    "#train_data = ratings_data_transformed[random_selection]\n",
    "#test_data = ratings_data_transformed[~random_selection]\n",
    "\n",
    "#train_data.to_csv(\"train_data_100K-v2.csv\", index=False, sep=\"|\", header=False)\n",
    "#test_data.to_csv(\"test_data_100K-v2.csv\", index=False, sep=\"|\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "115fd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADER = list(ratings_data_transformed.columns)\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"user_id\": list(users_df.user_id.unique()),\n",
    "    \"movie_id\": list(movies_df.movie_id.unique()),\n",
    "    \"gender\": list(users_df.gender.unique()),\n",
    "    \"age\": list(users_df.age.unique()),\n",
    "    \"job\": list(users_df.job.unique()),\n",
    "}\n",
    "\n",
    "USER_FEATURES = [\"gender\", \"age\", \"job\"]\n",
    "\n",
    "MOVIE_FEATURES = [\"genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af2fa485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    def process(features):\n",
    "        movie_ids_string = features[\"sequence_movie_ids\"]\n",
    "        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n",
    "\n",
    "        # The last movie id in the sequence is the target movie.\n",
    "        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n",
    "        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n",
    "\n",
    "        ratings_string = features[\"sequence_ratings\"]\n",
    "        sequence_ratings = tf.strings.to_number(\n",
    "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
    "        ).to_tensor()\n",
    "\n",
    "        # The last rating in the sequence is the target for the model to predict.\n",
    "        target = sequence_ratings[:, -1]\n",
    "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
    "\n",
    "        return features, target\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        field_delim=\"|\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(process)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78a72fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    return {\n",
    "        \"user_id\": tf.keras.layers.Input(name=\"user_id\", shape=(1,), dtype=tf.string),\n",
    "        \"sequence_movie_ids\": tf.keras.layers.Input(\n",
    "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=tf.string\n",
    "        ),\n",
    "        \"target_movie_id\": tf.keras.layers.Input(\n",
    "            name=\"target_movie_id\", shape=(1,), dtype=tf.string\n",
    "        ),\n",
    "        \"sequence_ratings\": tf.keras.layers.Input(\n",
    "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
    "        ),\n",
    "        \"gender\": tf.keras.layers.Input(name=\"gender\", shape=(1,), dtype=tf.string),\n",
    "        \"age\": tf.keras.layers.Input(name=\"age\", shape=(1,), dtype=tf.string),\n",
    "        \"job\": tf.keras.layers.Input(name=\"job\", shape=(1,), dtype=tf.string),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e7f8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def encode_input_features(\n",
    "    inputs,\n",
    "    include_user_id=True,\n",
    "    include_user_features=True,\n",
    "    include_movie_features=True,\n",
    "):\n",
    "\n",
    "    encoded_transformer_features = []\n",
    "    encoded_other_features = []\n",
    "\n",
    "    other_feature_names = []\n",
    "    if include_user_id:\n",
    "        other_feature_names.append(\"user_id\")\n",
    "    if include_user_features:\n",
    "        other_feature_names.extend(USER_FEATURES)\n",
    "\n",
    "    ## Encode user features\n",
    "    for feature_name in other_feature_names:\n",
    "        # Convert the string input values into integer indices.\n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "        idx = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
    "            inputs[feature_name]\n",
    "        )\n",
    "        # Compute embedding dimensions\n",
    "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "        # Create an embedding layer with the specified dimensions.\n",
    "        embedding_encoder = tf.keras.layers.Embedding(\n",
    "            input_dim=len(vocabulary),\n",
    "            output_dim=embedding_dims,\n",
    "            name=f\"{feature_name}_embedding\",\n",
    "        )\n",
    "        # Convert the index values to embedding representations.\n",
    "        encoded_other_features.append(embedding_encoder(idx))\n",
    "    \n",
    "    ## Create a single embedding vector for the user features\n",
    "    if len(encoded_other_features) > 1:\n",
    "        encoded_other_features = tf.keras.layers.concatenate(encoded_other_features)\n",
    "    elif len(encoded_other_features) == 1:\n",
    "        encoded_other_features = encoded_other_features[0]\n",
    "    else:\n",
    "        encoded_other_features = None\n",
    "\n",
    "    ## Create a movie embedding encoder\n",
    "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
    "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
    "    # Create a lookup to convert string values to integer indices.\n",
    "    movie_index_lookup = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=movie_vocabulary,\n",
    "        mask_token=None,\n",
    "        num_oov_indices=0,\n",
    "        name=\"movie_index_lookup\",\n",
    "    )\n",
    "    # Create an embedding layer with the specified dimensions.\n",
    "    movie_embedding_encoder = tf.keras.layers.Embedding(\n",
    "        input_dim=len(movie_vocabulary),\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name=f\"movie_embedding\",\n",
    "    )\n",
    "    \n",
    "    ######################################################## Create a vector lookup for movie genres.\n",
    "    movie_genres = movies_df[[\"Action\",\"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\",\n",
    "    \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]]\n",
    "    genre_vectors = movie_genres.to_numpy()\n",
    "\n",
    "    movie_genres_lookup = tf.keras.layers.Embedding(\n",
    "        input_dim=genre_vectors.shape[0],\n",
    "        output_dim=genre_vectors.shape[1],\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(genre_vectors),\n",
    "        trainable=False,\n",
    "        name=\"genres_vector\",\n",
    "    )\n",
    "    # Create a processing layer for genres.\n",
    "    movie_embedding_processor = tf.keras.layers.Dense(\n",
    "        units=movie_embedding_dims,\n",
    "        activation=\"relu\",\n",
    "        name=\"process_movie_embedding_with_genres\",\n",
    "    )\n",
    "\n",
    "    ## Define a function to encode a given movie id.\n",
    "    def encode_movie(movie_id):\n",
    "        # Convert the string input values into integer indices.\n",
    "        movie_idx = movie_index_lookup(movie_id)\n",
    "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
    "        encoded_movie = movie_embedding\n",
    "        if include_movie_features:\n",
    "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
    "            encoded_movie = movie_embedding_processor(\n",
    "                tf.keras.layers.concatenate([movie_embedding, movie_genres_vector])\n",
    "            )\n",
    "        return encoded_movie\n",
    "\n",
    "    ## Encoding target_movie_id\n",
    "    target_movie_id = inputs[\"target_movie_id\"]\n",
    "    encoded_target_movie = encode_movie(target_movie_id)\n",
    "\n",
    "    ## Encoding sequence movie_ids.\n",
    "    sequence_movies_ids = inputs[\"sequence_movie_ids\"]\n",
    "    encoded_sequence_movies = encode_movie(sequence_movies_ids)\n",
    "    # Create positional embedding.\n",
    "    position_embedding_encoder = tf.keras.layers.Embedding(\n",
    "        input_dim=sequence_length,\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name=\"position_embedding\",\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
    "    encodded_positions = position_embedding_encoder(positions)\n",
    "    # Retrieve sequence ratings to incorporate them into the encoding of the movie.\n",
    "    sequence_ratings = tf.expand_dims(inputs[\"sequence_ratings\"], -1)\n",
    "    # Add the positional encoding to the movie encodings and multiply them by rating.\n",
    "    encoded_sequence_movies_with_poistion_and_rating = tf.keras.layers.Multiply()(\n",
    "        [(encoded_sequence_movies + encodded_positions), sequence_ratings]\n",
    "    )\n",
    "\n",
    "    # Construct the transformer inputs.\n",
    "    for encoded_movie in tf.unstack(\n",
    "        encoded_sequence_movies_with_poistion_and_rating, axis=1\n",
    "    ):\n",
    "        encoded_transformer_features.append(tf.expand_dims(encoded_movie, 1))\n",
    "    encoded_transformer_features.append(encoded_target_movie)\n",
    "\n",
    "    encoded_transformer_features = tf.keras.layers.concatenate(\n",
    "        encoded_transformer_features, axis=1\n",
    "    )\n",
    "\n",
    "    return encoded_transformer_features, encoded_other_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ffb190f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vbert\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:2453: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "include_user_id = True\n",
    "include_user_features = True\n",
    "include_movie_features = True\n",
    "\n",
    "hidden_units = [64, 64, 64]\n",
    "\n",
    "dropout_rate = 0.1\n",
    "num_heads = 2\n",
    "\n",
    "def create_model():\n",
    "    inputs = create_model_inputs()\n",
    "    transformer_features, other_features = encode_input_features(\n",
    "        inputs, include_user_id, include_user_features, include_movie_features\n",
    "    )\n",
    "\n",
    "    # Create a multi-headed attention layer.\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
    "    )(transformer_features, transformer_features)\n",
    "\n",
    "    # Transformer block.\n",
    "    attention_output = tf.keras.layers.Dropout(dropout_rate)(attention_output)\n",
    "    x1 = tf.keras.layers.Add()([transformer_features, attention_output])\n",
    "    x1 = tf.keras.layers.LayerNormalization()(x1)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x2 = tf.keras.layers.Dense(units=x2.shape[-1])(x2)\n",
    "    x2 = tf.keras.layers.Dropout(dropout_rate)(x2)\n",
    "    transformer_features = tf.keras.layers.Add()([x1, x2])\n",
    "    transformer_features = tf.keras.layers.LayerNormalization()(transformer_features)\n",
    "    features = tf.keras.layers.Flatten()(transformer_features)\n",
    "\n",
    "    # Included the other features.\n",
    "    if other_features is not None:\n",
    "        features = tf.keras.layers.concatenate(\n",
    "            [features, tf.keras.layers.Reshape([other_features.shape[-1]])(other_features)]\n",
    "        )\n",
    "\n",
    "    # Fully-connected layers.\n",
    "    for num_units in hidden_units:\n",
    "        features = tf.keras.layers.Dense(num_units)(features)\n",
    "        features = tf.keras.layers.BatchNormalization()(features)\n",
    "        features = tf.keras.layers.LeakyReLU()(features)\n",
    "        features = tf.keras.layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=1)(features)\n",
    "        \n",
    "    ## Adding a Lambda layer to convert the output to rating by scaling it with the help of available rating information\n",
    "    max_rating = 5\n",
    "    min_rating = 1\n",
    "    x = tf.keras.layers.Lambda(lambda x: x*(max_rating - min_rating) + min_rating)(outputs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "257d6bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "303/303 [==============================] - 6s 10ms/step - loss: 2.0791 - root_mean_squared_error: 1.4419 - val_loss: 1.1675 - val_root_mean_squared_error: 1.0805\n",
      "Epoch 2/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 1.1828 - root_mean_squared_error: 1.0876 - val_loss: 1.0956 - val_root_mean_squared_error: 1.0467\n",
      "Epoch 3/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 1.1020 - root_mean_squared_error: 1.0498 - val_loss: 1.0477 - val_root_mean_squared_error: 1.0236\n",
      "Epoch 4/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 1.0294 - root_mean_squared_error: 1.0146 - val_loss: 0.9887 - val_root_mean_squared_error: 0.9944\n",
      "Epoch 5/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.9560 - root_mean_squared_error: 0.9778 - val_loss: 0.9098 - val_root_mean_squared_error: 0.9538\n",
      "Epoch 6/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.9213 - root_mean_squared_error: 0.9599 - val_loss: 0.9013 - val_root_mean_squared_error: 0.9493\n",
      "Epoch 7/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.9012 - root_mean_squared_error: 0.9493 - val_loss: 0.8882 - val_root_mean_squared_error: 0.9425\n",
      "Epoch 8/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8903 - root_mean_squared_error: 0.9436 - val_loss: 0.9079 - val_root_mean_squared_error: 0.9528\n",
      "Epoch 9/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8811 - root_mean_squared_error: 0.9387 - val_loss: 0.8780 - val_root_mean_squared_error: 0.9370\n",
      "Epoch 10/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8759 - root_mean_squared_error: 0.9359 - val_loss: 0.8742 - val_root_mean_squared_error: 0.9350\n",
      "Epoch 11/50\n",
      "303/303 [==============================] - 3s 10ms/step - loss: 0.8716 - root_mean_squared_error: 0.9336 - val_loss: 0.8747 - val_root_mean_squared_error: 0.9352\n",
      "Epoch 12/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8696 - root_mean_squared_error: 0.9325 - val_loss: 0.8794 - val_root_mean_squared_error: 0.9377\n",
      "Epoch 13/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8635 - root_mean_squared_error: 0.9293 - val_loss: 0.8728 - val_root_mean_squared_error: 0.9342\n",
      "Epoch 14/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8608 - root_mean_squared_error: 0.9278 - val_loss: 0.8738 - val_root_mean_squared_error: 0.9348\n",
      "Epoch 15/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8587 - root_mean_squared_error: 0.9267 - val_loss: 0.8692 - val_root_mean_squared_error: 0.9323\n",
      "Epoch 16/50\n",
      "303/303 [==============================] - 3s 10ms/step - loss: 0.8545 - root_mean_squared_error: 0.9244 - val_loss: 0.8695 - val_root_mean_squared_error: 0.9325\n",
      "Epoch 17/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8504 - root_mean_squared_error: 0.9221 - val_loss: 0.8679 - val_root_mean_squared_error: 0.9316\n",
      "Epoch 18/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8511 - root_mean_squared_error: 0.9226 - val_loss: 0.8651 - val_root_mean_squared_error: 0.9301\n",
      "Epoch 19/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8496 - root_mean_squared_error: 0.9218 - val_loss: 0.8654 - val_root_mean_squared_error: 0.9303\n",
      "Epoch 20/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8474 - root_mean_squared_error: 0.9206 - val_loss: 0.8682 - val_root_mean_squared_error: 0.9318\n",
      "Epoch 21/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8454 - root_mean_squared_error: 0.9195 - val_loss: 0.8646 - val_root_mean_squared_error: 0.9298\n",
      "Epoch 22/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8419 - root_mean_squared_error: 0.9175 - val_loss: 0.8701 - val_root_mean_squared_error: 0.9328\n",
      "Epoch 23/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8405 - root_mean_squared_error: 0.9168 - val_loss: 0.8764 - val_root_mean_squared_error: 0.9361\n",
      "Epoch 24/50\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 0.8388 - root_mean_squared_error: 0.9159 - val_loss: 0.8774 - val_root_mean_squared_error: 0.9367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d15b73070>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1), ## Adagrad, Adadelta work better\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# Read the training data.\n",
    "train_dataset = get_dataset_from_csv(\"train_data_100K.csv\", shuffle=True, batch_size=265)\n",
    "\n",
    "# Read the test data.\n",
    "test_dataset = get_dataset_from_csv(\"test_data_100K.csv\", batch_size=265)\n",
    "\n",
    "# Callbacks\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "# Fit the model with the training data.\n",
    "model.fit(train_dataset, epochs=50, batch_size = 8, verbose = 1, validation_data=test_dataset, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "861327d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_bst_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_bst_model\\assets\n"
     ]
    }
   ],
   "source": [
    "## Saves model\n",
    "model.save('my_bst_model')\n",
    "\n",
    "## Loads model\n",
    "#new_model = tf.keras.models.load_model('my_bst_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
