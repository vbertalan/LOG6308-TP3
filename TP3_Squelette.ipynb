{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbertalan/LOG6308-TP3/blob/main/TP3_Squelette.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi8yXoiIwwqL"
      },
      "source": [
        "# LOG6308\n",
        "# TP3 : Systèmes de recommandation et réseaux de neurones\n",
        "\n",
        "L'objectif du TP3 est de vous familiariser avec la librairie `Tensorflow` et `Tensorflow Recommenders`. Nous souhaitons aussi vous familiariser avec le concept de réseaux neuronaux.\n",
        "C'est pourquoi nous vous proposons d'effectuer des recommandations de films sur la base de données que vous connaissez bien maintenant : [MovieLens 100k](https://grouplens.org/datasets/movielens/).<br>\n",
        "\n",
        "Le TP sera noté **sur 100**.\n",
        "\n",
        "## Critères de correction\n",
        "\n",
        "- Démarche valide et bien expliquée\n",
        "- Réponses correctes et commentées\n",
        "- Présentation soignée \n",
        "- Choix de fonctionnalités adéquat\n",
        "- Interprétation étayée des résultats\n",
        "\n",
        "## Instructions Globales\n",
        "\n",
        "Le travail doit être fait en **équipe de deux**.\n",
        "\n",
        "Vous avez le droit d'utiliser **seulement** les **librairies importées** pour résoudre les **questions 1, 2 et 3**. Si vous utilisez d’autres librairies, vos réponses ne seront pas considérées.\n",
        "\n",
        "Vous pouvez répondre aux sous-questions en commentaire ou dans des cellules textes en prenant bien soin d’identifier à quelle question vous répondez.\n",
        "Ceux qui le souhaite peuvent développer en local et écrire votre code dans des fichiers Python en `.py`. Ceci dit, j'attends de vous un README.md m'expliquant comment exécuter votre code avec une liste de dépendances (Requirements).\n",
        "\n",
        "Pour les questions 1-2-3, le Notebook est suffisant. Vous pouvez marquer vos commentaires et réponses qualitatives dans des cellules textes. \n",
        "Par contre, pour la question 4, il est recommandé de fournir un rapport séparé du code en format PDF. Mais, si vous ne souhaitez pas rédiger de rapport, vous pouvez rédiger votre état de l’art et votre démarche dans des cellules textes du Notebook sur Colab.\n",
        "\n",
        "\n",
        "Pour la remise du travail sur Moodle, on s’attend à un Zip qui contient un notebook en `.ipynb` et/ou des fichiers Python en `.py`. **Si vous décidez** de **rédiger un rapport** pour la **question 4**, vous devez alors aussi **inclure** un fichier **PDF**. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQGIsPAV3XZu"
      },
      "source": [
        "### Comment télécharger le notebook\n",
        "\n",
        "- Cliquez sur le menu \"Fichier\" (*File* en anglais) dans le coin supérieur gauche.\n",
        "- Une fenêtre popup apparaît, trouvez `Télécharger -> Télécharger le fichier .ipynb` et cliquez dessus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIyu9ymF7loQ"
      },
      "source": [
        "#### Installation de tensorflow datasets, tensorflow recommenders, et importation des librairies requises pour le TP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iUJAKyxxAA-h",
        "outputId": "bed32206-1b1f-4411-ed3f-12d1665c8b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.5.2.dev202203140044-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.17.3)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.10.0.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n",
            "Collecting etils[epath-no-tf]\n",
            "  Downloading etils-0.4.0-py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath-no-tf]->tfds-nightly) (3.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.55.0)\n",
            "Installing collected packages: etils, toml, tfds-nightly\n",
            "Successfully installed etils-0.4.0 tfds-nightly-4.5.2.dev202203140044 toml-0.10.2\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 18.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.7 MB 5.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tfds-nightly\n",
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets==4.3\n",
        "!pip install -q scann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5Mylp7CACnE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbEAcYCo3quT"
      },
      "source": [
        "## Utilisation du GPU\n",
        "Les calculs seront plus rapides si vous utilisez le GPU. Ça sera particulièrement important pour la dernière partie. Pour s'assurer que le notebook utilise le GPU, vous pouvez modifier la configuration ainsi :\n",
        "* (EN) `Edit > Notebook Settings`\n",
        "* (FR) `Modifier > Paramètres du notebook`\n",
        "\n",
        "Par contre, faites attention à ne pas utiliser le GPU si vous n'en avez pas besoin. Colab limite le temps d'utilisation des GPUs pour sa version gratuite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p1o11-QT8-s9"
      },
      "outputs": [],
      "source": [
        "#Test CPU ou GPU\n",
        "if(len(tf.config.list_physical_devices('GPU')) == 0):\n",
        "    print(\"Vous utilisez actuellement le CPU\")\n",
        "else:\n",
        "    print(\"Vous utilisez actuellement le GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X51rl6S_7uyy"
      },
      "source": [
        "#### Téléchargement de MovieLens 100k\n",
        "\n",
        "Vous pouvez accéder à la documentation en appuyant sur ce [lien](https://www.tensorflow.org/datasets/catalog/movielens#movielens100k-ratings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zpXu3gy0AIqY"
      },
      "outputs": [],
      "source": [
        "# Les votes + des données supplémentaires\n",
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\", shuffle_files = False)\n",
        "# Les genres, titres et identifiants des films.\n",
        "films = tfds.load(\"movielens/100k-ratings\", split=\"train\", shuffle_files = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lkGxv6l6CYrU"
      },
      "outputs": [],
      "source": [
        "#tfds\n",
        "type(ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X3_idfqBzOl"
      },
      "source": [
        "Comme vous le voyez, `ratings` et `films` sont générés par `tfds` et sont des `tf.data.Dataset`. Pour avoir une idée de comment les utilisées, vous pouvez consulter ces liens : <br>\n",
        "- [DataSet](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
        "- [tfds](https://www.tensorflow.org/datasets/overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CiGdoK_iCFeF"
      },
      "outputs": [],
      "source": [
        "# Exemple d'utilisation\n",
        "list(ratings.map(lambda x: x[\"user_id\"]).take(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J9HUKr2Gf7a"
      },
      "source": [
        "## Question 1 (15 pts)\n",
        "Dans cette question, nous allons définir et entrainer un modèle dit *Two Tower* afin de prédire les votes selon cette formule :\n",
        "\n",
        "$$pred_{i,j}= b + E_{u_i}^TE_{f_j}.$$\n",
        "\n",
        "\n",
        "Où $(E_{u_i}, E_{f_j}) \\in \\mathbb{R}^n \\times \\mathbb{R}^n$ sont respectivement les plongements (<i>embeddings</i>) de l'utilisateur $i$, $u_i$, et du film $j$, $f_j$. De plus, $b \\in \\mathbb{R}$ est la constante qui représente la moyenne. Enfin, $n \\in \\mathbb{N}$ est respectivement la dimension de l'espace latent des utilisateurs et des films (dans cette question, $n=32$).\n",
        "\n",
        "<br>\n",
        "\n",
        "***Pour répondre aux questions, vous devez remplacer les `?` par les valeurs adéquates.***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Extraire les attributs nécessaires pour entrainer le modèle (1 pt)\n",
        "\n",
        "On vous demande d'extraire des données les `titres de films`, les `identifiants utilisateurs`, les `votes`, et les `horodatages` (<i>timestamps</i>). Les données doivent être sous format chaine de caractères (`str`). Voici la [doc](https://www.tensorflow.org/datasets/catalog/movielens#movielens100k-ratings). <br><br>\n",
        "*À modifier si vous voulez inclure d'autres features pour les questions 3 et 4.*"
      ],
      "metadata": {
        "id": "BXC_qIhL8wyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViGKneQPIq_t"
      },
      "outputs": [],
      "source": [
        "votes = ratings.map(lambda x: {\"?\": x[\"?\"],\"?\": x[\"?\"],\"?\": x[\"?\"], \"?\": tf.strings.as_string(x[\"?\"])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLTYHO8EJ_5v"
      },
      "source": [
        "### 1.2. Statistiques sur les données de `MovieLens 100k`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFzhe1s5KEHz"
      },
      "source": [
        "#### 1.2.a Affichez le nombre d'utilisateurs uniques (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05URFTTVJgQP"
      },
      "outputs": [],
      "source": [
        "#Titres des films\n",
        "titres_films    = films.map(lambda x: x[\"?\"]).batch(1000000)# On prend tous les films d'un coup\n",
        "films_unique    = np.unique(np.concatenate(list(titres_films)))\n",
        "nb_films_unique = films_unique.shape[0]\n",
        "nb_films_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFzgnvQ-KLj-"
      },
      "source": [
        "#### 1.2.b Affichez le nombre de films uniques (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OAIp2eSkKLsg"
      },
      "outputs": [],
      "source": [
        "#Identifiant des utilisateurs\n",
        "id_utilisateurs = votes.map(lambda x: x[\"?\"]).batch(1000000)# On prend tous les utilisateurs d'un coup\n",
        "id_uniques      = np.unique(np.concatenate(list(id_utilisateurs)))\n",
        "nb_id_uniques   = id_uniques.shape[0]\n",
        "nb_id_uniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqEdoGD6KL31"
      },
      "source": [
        "#### 1.2.c Affichez le nombre de votes et les fréquences de paires (utilisateurs, films) uniques. Constatez-vous des anomalies ? Si oui, quelles sont-elles ? (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cee_a332gHVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD5AjtAfJhCB"
      },
      "source": [
        "### 1.3. Initialisation de la metrique RMSE de tfrs (1 pt)\n",
        "\n",
        "Soit $y\\in \\mathbb{R}^N$ un vecteur de valeur de votes issue de la base de données d'entrainement, et $\\hat{y} \\in \\mathbb{R}^N$ la prédiction de ces votes par notre modèle. Pour que notre modèle soit performant, nous aimerions bien que $\\hat{y}$ ait quasiment les mêmes valeurs que $y$. On cherche donc à minimiser la perte suivante, qui est la **MSE** (*Mean Square Error*) :\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\boxed{l(\\hat{y}, y) = ||\\hat{y}-y||_2^2 = \\cfrac{1}{N}\\underset{i=1}{\\overset{N}{\\sum }}|\\hat{y} - y|^2}.$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Initialisez la tâche avec la perte adéquate en utilisant `tfrs.tasks.Ranking`, voici la [doc](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/tasks/Ranking). Il faut donc utiliser `tf.keras.losses.MeanSquaredError()` comme **perte**, et `tf.keras.metrics.RootMeanSquaredError()` comme **métrique**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk5le5DAJgUf"
      },
      "outputs": [],
      "source": [
        "task = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAFp2Qq8O0-Q"
      },
      "source": [
        "### 1.4. Définition du modèle Two Towers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGrNtpwDQc5X"
      },
      "source": [
        "#### 1.4.1. On définit la dimension de l'espace latent (taille des plongements) comme étant égale à 32. Pourquoi ne pas avoir choisi une dimension plus élevée ? (1 pt)<br>\n",
        "\n",
        "<u>Réponse</u> :<br>\n",
        "*Insérer votre réponse ici*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCrYSMNDQ1qY"
      },
      "outputs": [],
      "source": [
        "embedding_dimension = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIrlqfBZQ2hd"
      },
      "source": [
        "#### 1.4.2. Définir les couches de plongement pour les utilisateurs et les films (1 pt)\n",
        "\n",
        "Pour initaliser les espaces de plongements, vous pouvez vous aider de la documentation de [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding).<br>\n",
        "\n",
        "Pour comprendre `tf.keras.layers.experimental.preprocessing.StringLookup`, aidez-vous de la [doc](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjVKI9T7Q2zf"
      },
      "outputs": [],
      "source": [
        "def initialisation_embeddings(embedding_dimension, id_uniques, films_unique):\n",
        "    user_model = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=?, mask_token=None),\n",
        "                                    tf.keras.layers.Embedding(len(?) + 1,# Le +1 représente la constante $c$\n",
        "                                                                ?)], name=\"User_Embedding\")\n",
        "\n",
        "    movie_model = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=?,mask_token=None),\n",
        "                                    tf.keras.layers.Embedding(len(?) + 1, \n",
        "                                                                ?)], name=\"Movie_Embedding\")  \n",
        "    return user_model, movie_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06nJZzs4Uivf"
      },
      "source": [
        "#### 1.4.3. Assemblez le modèle *Two Towers* (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8STmaQyWO0MX"
      },
      "outputs": [],
      "source": [
        "class MovieLensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, embedding_dimension, id_uniques, films_unique, task):\n",
        "    super().__init__()\n",
        "    self.user_model, self.movie_model = initialisation_embeddings(embedding_dimension, id_uniques, films_unique)\n",
        "    \n",
        "    self.pred = tf.keras.layers.Dot(axes=1)\n",
        "    \n",
        "    self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "  def call(self, features):\n",
        "    # TO DO\n",
        "    pass\n",
        "\n",
        "  \n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    \n",
        "    return self.task(labels=features[\"user_rating\"], predictions=self.call(features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m1Y-ELtVo4L"
      },
      "source": [
        "### 1.5. Entrainement du modèle\n",
        "Dans cette partie, on entraine et test le modèle defini au dessus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wXJezITRCGk"
      },
      "source": [
        "#### Définir les bases de données d'entrainement et de validation (proportion $80\\%-20\\%$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZI0GidMW9ju"
      },
      "outputs": [],
      "source": [
        "N          = len(votes)\n",
        "batch_size = 8192 #2^13\n",
        "prop       = 0.8\n",
        "train_len  = tf.cast(N * prop, dtype=tf.int64)\n",
        "valid_len   = tf.cast(N - train_len, dtype=tf.int64)\n",
        "\n",
        "\n",
        "# shuffled = votes.shuffle(N, seed=73, reshuffle_each_iteration=False)\n",
        "\n",
        "tf.random.set_seed(73)\n",
        "train = votes.take(train_len).shuffle(train_len, seed=73, reshuffle_each_iteration=False).batch(batch_size)\n",
        "valid = votes.skip(train_len).take(valid_len).shuffle(valid_len, seed=73, reshuffle_each_iteration=False).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsmpzsWYa_kU"
      },
      "source": [
        "#### 1.5.1. Initialisez le modèle, l'optimiseur et les modules de callback pour l'entrainement (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "86HwCAJoiJxh"
      },
      "outputs": [],
      "source": [
        "# On tire un exemple pour construire le graphe du modèle\n",
        "feature = next(iter(train))\n",
        "feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N0pn6SPTghMQ"
      },
      "outputs": [],
      "source": [
        "# On construit et affiche le modèle\n",
        "Model = MovieLensModel(?, ?, ?, ?)\n",
        "Model(feature)\n",
        "Model.summary() # comment expliquez-vous le nombre de paramètres des couches embeddings ? (32*x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R2s3ownsuPh"
      },
      "source": [
        "On utilise comme optimiseur `Adam` (voir la [doc](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)) qui prend $0.01$ comme valeur pour son `learning_rate`. On vous demande aussi d'utiliser la stratégie *early stopping* pour entrainer votre modèle (voir les explications [ici](https://www.educative.io/edpresso/what-is-early-stopping)). Cette stratégie est implémentée par `Keras` comme un module *callback*, voir la [doc](https://keras.io/api/callbacks/). La **patience** doit être égale à $3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13XVbRv-gh-R"
      },
      "outputs": [],
      "source": [
        "# Création du dossier contenant les modèles entrainés\n",
        "!mkdir Models/\n",
        "\n",
        "# Compiler le modèle en ajoutant l'optimiseur Adam\n",
        "Model.compile(optimizer=?)\n",
        "\n",
        "my_callbacks = [\n",
        "    ?,\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaXK27Rla_nI"
      },
      "source": [
        "#### 1.5.2. Entrainez le modèle sur **15 epochs** et afficher les résultats ainsi que la meilleure **RMSE** sur l'ensemble de validation. Y a-t-il surapprentissage ? (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hQiWAXEEjshI"
      },
      "outputs": [],
      "source": [
        "#Entrainement du modèle sur 15 epochs\n",
        "history_TwoTowers = Model.fit(?, epochs=?, validation_data=?, callbacks=?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "574bL-vDkGiS"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, model_name=\"Two Towers\"):\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['?'])\n",
        "    plt.plot(history.history['?'])\n",
        "    plt.title(model_name + ' Model RMSE')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='best')\n",
        "    plt.show()\n",
        "    print(\"\\n\\nBest RMSE on validation : {0:.4f}\".format(min(history.history['?'])))\n",
        "\n",
        "plot_history(history_TwoTowers, model_name=\"Two Towers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'overfitting n'a pas lieu de manière flagrande pendant les 15 premières epochs"
      ],
      "metadata": {
        "id": "4b5Zn9sw_LAw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwSZ9vowcBmX"
      },
      "source": [
        "## Question 2 (15 pts)\n",
        "\n",
        "Modifier le modèle Two Tower pour prendre en compte le biais utilisateur et item (film). La nouvelle formule de prédiction est donc :\n",
        "\n",
        "$$pred_{i,j}= \\sigma(b + biais_{u_i} + biais_{f_j} +E_{u_i}^TE_{f_j}) \\times (M_{vote} - m_{vote}) + m_{vote}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Où $biais_{u_i} \\in \\mathbb{R}$ est le biais associé à l'utilisateur $u_i$ et $biais_{f_j} \\in \\mathbb{R}$ le biais associé au film $f_j$. <br>\n",
        "\n",
        "$\\sigma: x \\mapsto \\cfrac{1}{1+e^{-x}}$ est la fonction sigmoid, elle est déjà implémentée par TensorFlow : `tf.math.sigmoid`.<br>\n",
        "\n",
        "Et, $M_{vote}, m_{vote}$ sont respectivement le maximum et le minimum des votes utilisateurs. Dans notre cas, $M_{vote}=5$ et $m_{vote}=1$.\n",
        "\n",
        "### Description du modèle Two Tower avec Biais\n",
        "\n",
        "Le modèle Keras correspondant est légèrement plus complexe. En plus des plongements d'utilisateurs et de films avec lesquelles nous avons déjà travaillé, le modèle ci-dessous approxime le biais utilisateur ($biais_{u_i}$) et le biais film ($biais_{f_j}$) en plongeant l'utilisateur et le film dans un espace unidimensionnel. Nous ajoutons ensuite les deux biais au produit scalaire représentant l'interaction utilisateur-film. La fonction d'activation sigmoïde normalise et ramène la prédiction à l'intervalle $[0,1]$, qui est ensuite ramenée à l'intervalle de vote original $[m_{vote}, M_{vote}]$. D'ailleurs, le dropout doit être appliqué aux sorties des couches `user_model` et `movie_model`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "331CwGWJHdhs"
      },
      "source": [
        "### Définissez, initialisez, entrainez, affichez et interprétez les résultats du modèle Two Tower modifié. Y a-t-il surapprentissage ?\n",
        "\n",
        "Dans cette question, il vous ai conseillé d'utiliser Adam avec un `learning_rate`$=0.005$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRkKTjY_cgRZ"
      },
      "outputs": [],
      "source": [
        "#MovieLensModelWithBias Herite des attributs et des méthodes de MovieLensModel\n",
        "class MovieLensModelWithBias(MovieLensModel):\n",
        "\n",
        "  def __init__(self, embedding_dimension, id_uniques, films_unique, task, min_vote=1, max_vote=5):\n",
        "    super().__init__(embedding_dimension, id_uniques, films_unique, task)\n",
        "\n",
        "    self.min_vote, self.max_vote = min_vote, max_vote\n",
        "\n",
        "    # Cette couche plonge dans un espace de dimension 1. Sa sortie est une constante qui représente le biais utilisateur.\n",
        "    self.user_bias = user_model = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=id_uniques, mask_token=None),\n",
        "                                    tf.keras.layers.Embedding(len(id_uniques) + 1, 1)], name=\"User_Bias\")\n",
        "\n",
        "    self.movie_bias = user_model = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=films_unique, mask_token=None),\n",
        "                                    tf.keras.layers.Embedding(len(films_unique) + 1, 1)], name=\"Movie_Bias\")\n",
        "\n",
        "    self.user_dropout  = tf.keras.layers.Dropout(rate = 0.3, name=\"User_Dropout\")\n",
        "    self.movie_dropout = tf.keras.layers.Dropout(rate = 0.6, name=\"Movie_Dropout\")\n",
        "\n",
        "\n",
        "  def call(self, features):\n",
        "    # TO DO \n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "83ADzoELml6x"
      },
      "outputs": [],
      "source": [
        "#Initialisez le modèle et afficher ses couches (summary)\n",
        "Model_2 = ?\n",
        "Model_2(feature)\n",
        "Model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV9_n_jKm97A"
      },
      "outputs": [],
      "source": [
        "# Compilez le modèle en ajoutant l'optimiseur Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "slPoB57hnAUi"
      },
      "outputs": [],
      "source": [
        "#Entrainez le modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8jgK6TUnBvu"
      },
      "outputs": [],
      "source": [
        "#Affichez les résultats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations\n",
        "\n",
        "<u>Réponse</u> :<br>\n",
        "\n",
        "*Répondez ici*"
      ],
      "metadata": {
        "id": "S4D0V1wgiaB6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p4rXQGoOQJp"
      },
      "source": [
        "## Question 3 (20 pts)\n",
        "Dans cette question, nous cherchons à améliorer le modèle Two Towers avec les biais de la question 2. <br> \n",
        "\n",
        "Voici quelques idées d'améliorations : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjXxt35mEAna"
      },
      "source": [
        "### Question 3.1 (10 pts)\n",
        "\n",
        "Améliorez les performances en changeant les hyperparamètres du modèle (<i>dropout, embedding_dim, learning rate, etc...</i>). Quelle est l'impact de ces hyperparamètres sur le surapprentissage (<i>overfitting</i>) ? **(10 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgxvThAaEDdn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAH7M7HTEF34"
      },
      "source": [
        "### Question 3.2 (10 pts)\n",
        "\n",
        "Commencez l'entrainement du modèle avec des plongements pré-entrainés (pretrained embeddings) obtenus aux questions précédentes. **(10 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8XJWNilEjWch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-OnVMmiJ7to"
      },
      "source": [
        "### Bonus (10 pts)\n",
        "\n",
        "Prenez en compte les `timestamps`, ou développez d'autres idées que vous détaillerez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp3wN9ZDoEdR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFtv1UQ_7f1d"
      },
      "source": [
        "## Question 4 (50 points)\n",
        "\n",
        "Maintenant que vous vous êtes familiarisés avec les librairies de `Tensorflow`, attaquons-nous à l'état de l'art. En utilisant des mots-clés comme `Deep Learning`, `Recommender Systems`, et `MovieLens`, faites une brève revue de l'état de l'art. Il est impératif que vous <b>citiez vos [sources](https://ulyngs.github.io/oxforddown/cites-and-refs.html)</b>.\n",
        "\n",
        "Ensuite, inspirez-vous de vos recherches pour proposer une approche plus performante que celle vue au-dessus. Pour cette question, il est recommandé de fournir un rapport séparé pour votre état de l'art et l'explication de votre démarche en format PDF. Mais, si vous ne souhaitez pas rédiger de rapport, vous pouvez rédiger dans les cellules textes ci-dessous.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Cette question vous laisse beaucoup de liberté dans vos réponses, vous pouvez utiliser n'importe quelle bibliothèque Python contrairement aux autres questions. Néanmoins, vous êtes quand même **soumis à des contraintes** :\n",
        "- Vous devez utiliser uniquement les données `MovieLens 100k`\n",
        "- **Si** vous **n**'avez **pas** besoin de features supplémentaires de `Movielens 100k` que celle extraite dans la **question 1.1**, utilisez l'ensemble d'entrainement (`train`) et de validation (`valid`) créée à la question 1.5\n",
        "- Votre modèle doit se baser sur des réseaux de neurones\n",
        "- Votre modèle doit être en tensorflow\n",
        "- Vous **ne** pouvez **pas** **entrainer** vos modèles sur les données de **validation** \n",
        "- Citez obligatoirement vos sources !\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Une approche possible qu'on vous propose est de **réimplémenter** la méthode décrite dans le papier [Scalable deep learning-based recommendation systems](https://www.sciencedirect.com/science/article/pii/S2405959518302029) de H. Lee et al.<br>\n",
        "\n",
        "Pour cela il faut :\n",
        "1. Créer la matrice utilisateur-item\n",
        "2. Implémenter leur preprocessing sur la matrice utilisateur-item\n",
        "3. Implémenter le modèle décrit pour `MovieLens 100k`\n",
        "4. Entrainer le modèle\n",
        "5. Comparer les résultats obtenus en calculant la RMSE sur l'ensemble de validation par rapport à ceux obtenus par les méthodes précédentes\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<big><b><center>Les 3 groupes ayant les meilleurs RMSE sur l'ensemble de validation auront 10 points de bonus.</center></b></big>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "**Qualités attendues du travail**, vous serez noté selon :\n",
        "- L'originalité de votre démarche\n",
        "- La cohérence de votre démarche avec l'état de l'art rédigé\n",
        "- Les résultats empiriques (**RMSE**) sur l'ensemble de validation, notamment est-ce qu'elle performe mieux que les méthodes précédentes de manière consistante ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-AyqT3spxFI"
      },
      "source": [
        "#### État de l'art (15 points)\n",
        "\n",
        "\n",
        "\n",
        "<u>Réponse :</u><br>\n",
        "<i>Vous pouvez rédiger ici.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ddey_xSp0uI"
      },
      "source": [
        "#### Code et démarche (35 points)\n",
        "\n",
        "##### Démarche et raisonnement :\n",
        "\n",
        "<ul>\n",
        "  <li>Citez vos sources qui vous aider à produire votre solution. En particulier, mentionnez la source du code que vous avez pris et modifié s'il y a lieu</li>\n",
        "  <li>Expliquez votre démarche et votre raisonnement.</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "<u>Réponse :</u><br>\n",
        "<i>Vous pouvez rédiger ici.</i>\n",
        "\n",
        "##### Code\n",
        "\n",
        "Faites en sorte que le code soit lisible et facilement interprétable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5srvX6wnFRVb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TP3_Squelette.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}